diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index 120b3c2d1..304a55a9e 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -57,6 +57,9 @@ from vllm.utils.torch_utils import direct_register_custom_op
 
 logger = init_logger(__name__)
 
+# Logging counter for MOE routes
+_MOE_COUNTER = 0
+
 
 @triton.jit
 def write_zeros_to_output(
@@ -1614,6 +1617,38 @@ def fused_experts_impl(
     w1_bias: torch.Tensor | None = None,
     w2_bias: torch.Tensor | None = None,
 ) -> torch.Tensor:
+   # --- START ASSIGNMENT PATCH ---
+    log_path = os.environ.get("VLLM_LOG_MOE")
+    if log_path:
+        global _MOE_COUNTER
+        current_layer = _MOE_COUNTER % 60 # Qwen1.5-MoE has 60 layers
+        target_layer = int(os.environ.get("VLLM_LOG_MOE_LAYER", "0"))
+
+        if current_layer == target_layer:
+            # Write meta header if file is empty/new
+            if not os.path.exists(log_path) or os.path.getsize(log_path) == 0:
+                import vllm
+                header = {
+                    "type": "meta", "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat",
+                    "vllm_version": getattr(vllm, "__version__", "0.0.0"),
+                    "torch_version": torch.__version__, "device": torch.cuda.get_device_name(0),
+                    "seed": 1234, "layers_logged": [target_layer], "top_k": topk_ids.size(1)
+                }
+                with open(log_path, "w") as f:
+                    f.write(json.dumps(header) + "\n")
+
+            # Extract routing data per token
+            t_ids = topk_ids.detach().cpu().tolist()
+            t_weights = topk_weights.detach().cpu().tolist()
+            with open(log_path, "a") as f:
+                for i in range(len(t_ids)):
+                    f.write(json.dumps({
+                        "type": "route", "req_id": "r1", "token_idx": i,
+                        "layer": target_layer, "topk_ids": t_ids[i],
+                        "topk_weights": [round(w, 4) for w in t_weights[i]]
+                    }) + "\n")
+        _MOE_COUNTER += 1
+    # --- END ASSIGNMENT PATCH ---
     # Check constraints.
     if use_int4_w4a16:
         assert hidden_states.size(1) // 2 == w1.size(2), "Hidden size mismatch"
@@ -1989,6 +2024,44 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):
         expert_tokens_meta: mk.ExpertTokensMetadata | None,
         apply_router_weight_on_input: bool,
     ):
+        # --- START ASSIGNMENT PATCH ---
+        log_path = os.environ.get("VLLM_LOG_MOE")
+        if log_path:
+            global _MOE_COUNTER
+            # Qwen1.5-MoE has 60 MoE layers
+            current_layer = _MOE_COUNTER % 60
+            target_layer = int(os.environ.get("VLLM_LOG_MOE_LAYER", "0"))
+
+            if current_layer == target_layer:
+                # 1. Write Meta Header
+                if not os.path.exists(log_path) or os.path.getsize(log_path) == 0:
+                    import vllm
+                    header = {
+                        "type": "meta", 
+                        "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat",
+                        "vllm_version": getattr(vllm, "__version__", "unknown"),
+                        "torch_version": torch.__version__, 
+                        "device": torch.cuda.get_device_name(0),
+                        "seed": 1234, 
+                        "layers_logged": [target_layer], 
+                        "top_k": topk_ids.size(1)
+                    }
+                    with open(log_path, "w") as f:
+                        f.write(json.dumps(header) + "\n")
+
+                # 2. Log Routing Data
+                t_ids = topk_ids.detach().cpu().tolist()
+                t_weights = topk_weights.detach().cpu().tolist()
+                with open(log_path, "a") as f:
+                    for i in range(len(t_ids)):
+                        f.write(json.dumps({
+                            "type": "route", "req_id": "r1", "token_idx": i,
+                            "layer": target_layer, "topk_ids": t_ids[i],
+                            "topk_weights": [round(w, 4) for w in t_weights[i]]
+                        }) + "\n")
+            
+            _MOE_COUNTER += 1
+        # --- END ASSIGNMENT PATCH ---
         # Check constraints.
         if self.quant_config.use_int4_w4a16:
             assert hidden_states.size(-1) // 2 == w1.size(2), "Hidden size mismatch"
